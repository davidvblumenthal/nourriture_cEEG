## Abstract
This study presents an approach to enhance the objective monitoring of food intake using cEEGrids. A 3D-printed around-the-ear electrode array was used to acquire data of participants eating different types of food as well as baseline activities. Signals are processed using different algorithms to create feature sets and to recognize patterns of the different featured activities. The results showed that Random Forests performed best on our data set with average accuracy and f1-score of 85.7% and 85.9%. This approach can be useful to automate and objectively measure ingestive behavior. An imaginable application would be in the treatment of overweight or anorexic patients. It could also be applicable to support Alzheimer's patients, as this disease is also often accompanied by a lack of food and fluid intake.

## Introduction
One of our biggest health problems as a modern society is obesity. Although the World Health Organisation (WHO) has identified obesity as an global epidemic since 2000, the proportion of overweight people is steadily increasing. According to a 2014 study of the German health institution Robert Koch Institut (RKI), about two-thirds of German men and around half of the women are overweight (23% men & 24% women are highly overweight). This, however, as indicated by the WHO, is not specific to Germany. The OECD 20 countries have an obesity proportion of 59,6 %. The USA has the third-highest obesity rate worldwide at 73,1%.

Obesity is characterized by the accumulation of excess body fat and can be seen as the consequence of a positive energy balance. Due to its far-reaching effects, obesity causes various serious medical complications such as type 2 diabetes, strokes, and coronary heart diseases. Apart from individual sickness, consequential illnesses put immense financial pressure on health and social systems. Although the formation of obesity is more complex than excess energy consumption relative to energy expenditure, excessive food intake and insufficient physical activity are the main known causes. Recent changes in food availability and advertisement have encouraged the overconsumption of high-calorie and high-fat foods, easily accessible through numerous fast-food restaurants. This offer of unhealthy food is further enriched by grocery stores that offer a broad selection of "convenience" food that is highly processed and full of sugar. In combination with modern technological advances that made tasks more efficient and consequently reduced the need for physical activity, the shifts in eating behavior increase the risk of developing unhealthy patterns.

Both food intake and exercising are essential for maintaining energy balance in the human body. Therefore, to have the best conditions for a healthy life, eating and exercising in the right proportions is essential. The monitoring of exercising is already well researched, and various studies exist. Practical applications and devices have emerged (e.g., Smart Watches and Fitness Trackers) and introduced a method to measure and quantify physical activity objectively. 
However, methods on the objective monitoring of food intake are lacking behind. To date, food records, food frequency questionnaires, as well as 24-h recalls are the most common methods used in treatment. Nevertheless, these methods share the same inherent problems. They rely on self-reported data, which is subjective and therefore, due to biases, often inaccurate. 

This study aims to present an approach to detect food intake activity using an around-the-ear electrode array. These so-called cEEGrids offer possibilities to capture EEG data in an unobtrusive and concealed way. To evaluate the approach, we investigate the following research question in a controlled environment: Can a machine learning classifier distinguish between eating different foods (nuts, sandwiches, and soup) and separate them from defined baseline activities. We defined an experiment where data of different activities is collected, analyzed and classified using different machine learning algorithms. The study then discusses achieved results and assesses whether this approach has the potential to improve the treatment of diseases related to dysfunctional food intake. Finally, this study concludes with an outlook and suggests further research directions. 

## Method
The OpenBCI platform offers and creates open-source tools for bio-sensing and neuroscience. They strive to lower the barrier to entry the brain-computer interfacing domain. They offer hardware (Cyton board and Daisy shield) that enables the low-cost mobile recording of biosignals on 18 channels. Two channels are used for reference and ground electrodes. cEEGrids, on the other hand, are flexible, printed Ag/AgCI electrodes positioned in a c-shaped array that fits around the ear. These sensors are designed to be unobtrusive and convenient to acquire EEG data under field conditions. They produce reliable, high-quality EEG recordings over several hours. Combining these two devices forms a Recording System that allows for a less expensive and flexible recording solution due to its open-source nature. The solution reduces costs from 10.000 USD to somewhere around 1.500 USD.

In this chapter, we briefly outline our applied research method. First, we started with the design of the experiment, followed by the collection of data using the illustrated recording system. Then a brief description of data pre-processing is followed by a section clarifying the model selection.

### Data Collection
To gather data for evaluation, we designed an experiment and implemented the different tasks in PsychoPy, a software solution for creating experiments. Participants conducting the experiment are asked to eat soup, sandwiches, and nuts. Reading a text out loud, yawning, resting with eyes open, resting with eyes closed were implemented as baseline activities. PsychoPy recorded the timestamps of each activities' start and end. ![[experiment.png]]
As shown in Figure above participants were asked to read a text out loud for 40 seconds. The routine is repeated twice with a break of 4 seconds after each routine respectively. Furthermore, the experiment consists of yawning 20 times for 3 seconds each and resting for 40 seconds, one time with open eyes, one time with closed eyes. Beside the baseline activities, subjects were asked to eat soup once for 40 seconds, nuts and sandwiches for two times 40 seconds each.  Due to restrictions caused by COVID-19, only two participants took part in that experiment. To generate enough data to train different models, each participant took part twice. Both participants were male, aged 25 and 26. 

As shown in Figure  participants were connected to an OpenBCI Board. To gather the data, we used the setup designed by   in their research of facial activity recognition. A combination of cEEGrids and OpenBCI amplifiers was used to obtain 16 recording channels. ![[IMG_6931.jpg]]
### Preprocessing
The recording files were loaded and analyzed using Python. Each channel signal was first mean-centered, then band-pass (FIR 1-62 Hz) and notch (50 Hz) filtered. The continuous signal was cut into the individual activities using the timestamps generated by PsychoPy. The resulting activity measurements were then epoched using non-overlapping one-second windows (125 data points per epoch). As features we choose the sum, maximum, Hurst-exponent (HE), Petrosian Fractal Dimension (PFD), the Higuchi Fractal Dimension (HFD) and the Hjorth parameters Activity, Mobility, and Complexity. HE, HFD, and the Hjorth parameters Complexity and Mobility have been calculated using the python library mne_features. The PFD and Hjorth Activity have been calculated using the python library eeglib.

The mentioned features were calculated for each epoch and each channel (16), resulting in a first feature set $FS1_{-signal}$ containing 1840 samples with 128 features. To generate a second feature set $FS2_{-signal+}$ we calculated the first 3 individual components for the whole signal. Individual components are considered to capture the main muscle activity. Following, we calculated the same features as in $FS1_{-signal}$, which resulted in 1840 samples with 152 features.

Due to the unequal length of the different activities, the class frequencies are unevenly distributed, which means we were dealing with an unbalanced data set. The distribution is visualized below. 
![[class_distribution.png]]
To account for the data set imbalance we used Synthetic Minority Oversampling (SMOTE), which generates a balanced data set by synthesizing new samples for the minority class. After applying the algorithm, the data set was balanced containing exactly 320 samples of each class and 2240 samples in total. The constructed data set was saved as a pickle file. The pickle file could then be imported for model-specific prepossessing and training.

### Model selection
After preprocessing the data, Random Forest, Support Vector Machine, AdaBoost, Logistic Regression, and a Multi-layer Perceptron were tested as classifiers. In our research, we focused on comparing the performance of these different classifiers rather than optimizing them. 
 
The Random Forest classifier can be characterized as an ensemble learning technique using decision trees. Ensemble learning algorithms combine different models and therefore lead to higher prediction performance. Random Forests use two approaches to create randomness. On the one hand, just a subset of data is chosen for each decision tree. On the other hand, the structure of each decision tree is random as well, as the attributes of each tree are also chosen randomly. The classification result is then determined by a majority vote of the individual decision trees. Due to inherent randomness in this algorithm, it is more robust to outliers and has a lower risk of overfitting the training data.
The creation of smaller trees leads to faster training but increases proportionally to the number of trees.

Support Vector Machine (SVM) is an approach to separate classes in a multidimensional space. The SVM calculates the optimal hyperplane in a given space to separate two or more classes. For non-linear problems, SVMs use a technique called "kernel trick." This technique transforms the data in a higher-dimensional space and can again calculate the optimal hyperplanes. As the kernel trick uses kernel functions, SVMs can be used for different applications and are effective in high dimensions. Downsides are the need for external preprocessing, as well as parameter optimization for the optimal kernel.
 
AdaBoost trains and combines several weak classifiers to receive a strong model. AdaBoost draws a random subset of data from training data. The algorithm then applies weights to each data point of the random subset. In case of a correct classification, weights will be increased, respectively decreased for false classifications. Weight is also applied for every single classifier so that the final classification from AdaBoost concludes from majority voting of the weighted ensemble of weak classifiers. Usually, AdaBoost performs well without costly parameter tuning. On the contrary, AdaBoost has its limitations in noisy data. 
 
Multi-layer Perceptron (MLP) consists of multiple, connected layers of neurons. In general, three different types of layers exist—the input layer, hidden layers, and output layer. The input layer receives the weighted data and forwards it through one or multiple hidden layers to the output layer, resulting in a classification. Each neuron contains a non-linear activation function, which performs a single output. Based on the activation of neurons, the Multi-layer Perceptron is trained using a gradient descent approach, where an error on predictions is calculated and propagated backward through the network. The weight of each neuron will then be adjusted. MLPs perform well with large and complex input data, but computations can be redundant, inefficient, and computationally costly. 
 
Logistic Regression uses a logistic function to classify input data. The output prediction function returns a probability between 0 and 1. Logistic Regression has a threshold for the classification. For probabilities larger or equal to the threshold Logistic Regression will classify the output as positive, while classifying the output negative with a lower probability than the threshold. Logistic regression is easy to implement and train. The algorithm expects pre-processed data, which increases the computational costs of the overall classification. In addition, Logistic Regression is bound to a linear decision as it is based on a linear model. 

### Model evaluation
Models can be evaluated using stratified k-fold cross-validation. Stratified k-fold cross-validation shuffles the data set and splits the data in k folds. During the first iteration, the model is trained using k-1 folds, whereas one fold is used to evaluate the model. Each iteration uses a different set of training and evaluation data, resulting in an accurate performance estimation. It can be concluded, that the performance of the model is less biased by the selection of specific train or validation data.

## Results
We used 5-fold cross-validation to evaluate the performance of the previously introduced algorithms. Since precision and recall have a trade-off, we analyzed accuracy and f1-score to inspect if both precision and recall showed a sufficient result. The classifiers were trained for both subjects together, there was no differentiation between participants. 
Comparing Table [1] and Table [2], classification results improved when using SMOTE.
| ****               | **Random Forest** | **SVM** | **MLP** | **AdaBoost** | **Logistic Regression** |
|--------------------|-------------------|---------|---------|--------------|-------------------------|
| **FS1 (accuracy)** | 79.8%             | 78.9%   | 78.0%   | 49.0%        | 77.7%                   |
| **FS2 (accuracy)** | 79.8%             | 79.2%   | 79.2%   | 51.0%        | 77.7%                   |
| **FS1 (f1-score)** | 78.6%             | 77.5%   | 78.0%   | 41.0%        | 77.4%                   |
| **FS2 (f1-score)** | 79.9%             | 77.8%   | 79.1%   | 41.2%        | 77.5%                   |
On the basis of the unbalanced data set, Random Forest showed the best performance, as shown in Table [1]. The accuracy was 79.8% for both $FS1_{-signal}$ and $FS2_{-signal+}$. The f1-score improved 0.3% to 79.9% using $FS2_{-signal+}$ instead of $FS1_{-signal}$. In general using $FS2_{-signal+}$ instead of $FS1_{-signal}$ leads to a greater or equal accuracy and f1-score, with only AdaBoost as an exception. The SVM and MLP show both nearly 80% accuracy as well, both for $FS1_{-signal}$ and $FS2_{-signal+}$. The MLP had a slightly bigger increase in accuracy with $FS2_{-signal+}$ of 1.2% to 79.2%, compared to the SVM. Logistic Regression reached 77.7% in accuracy for both $FS1_{-signal}$ and 2. The f1-score increased by 0.1% to 77.5% using $FS2_{-signal+}$. AdaBoost had the lowest performance among all classifiers for accuracy, with 44.2% and 41.7% for $FS1_{-signal}$ and $FS2_{-signal+}$, as well as for the f1-score, with 35.2% and 33.1% for $FS1_{-signal}$ and $FS2_{-signal+}$.

| ****               | **Random Forest** | **SVM** | **MLP** | **AdaBoost** | **Logistic Regression** |
|--------------------|-------------------|---------|---------|--------------|-------------------------|
| **FS1 (accuracy)** | 85.0%             | 82.5%   | 84.2%   | 44.2%        | 82.2%                   |
| **FS2 (accuracy)** | 85.7%             | 82.8%   | 84.4%   | 41.7%        | 82.9%                   |
| **FS1 (f1-score)** | 84.6%             | 82.3%   | 84.0%   | 35.2%        | 82.1%                   |
| **FS2 (f1-score)** | 85.9%             | 82.7%   | 82.9%   | 33.1%        | 82.8%                   |

As shown in Table [2], Random Forest, Support Vector Machine, Multilayer Perceptron, and Logistic Regression had an accuracy and f1-score of above 80%, both for $FS1_{-signal}$ and $FS2_{-signal+}$ in the oversampled data set. Solely AdaBoost had accuracy scores of 44.2% and 41.7% for $FS1_{-signal}$ and $FS2_{-signal+}$. 
In general $FS2_{-signal+}$ led to higher accuracy and f1-score. Exceptions are the f1-score of the Multilayer Perceptron and the accuracy and f1-score of the AdaBoost. 
The best performance among all classifiers yields the Random Forest, with an accuracy of 85.0% and 85.7% for $FS1_{-signal}$ and $FS2_{-signal+}$. The f1-score is 84.6% and 85.9% for $FS1_{-signal}$ respectively $FS2_{-signal+}$. 

In Figure Figure below the classification results of the Random Forest classifier are represented in a normalized confusion matrix. The correct classified classes are plotted diagonally. The activity speaking, was assigned the wrong label resting closed in 21 % of the cases and represents the weakest classification result amongst all activities. Further, resting with open eyes and yawning have a confusion rate of 11% and 8%. Especially the eating activities were separated well, with a correct classification of above 90%. There appeared to be little confusion amongst the individual eating activities. Eating sandwiches and eating nuts yield a correct classification of 93%, eating soup was classified with 92% correct. ![[confusion.png]]
## Discussion
Research on classifying food intake is still in its early stages, yet it can become an essential factor for obesity treatment. Our research started with an initial classification of different solid foods and non-solid food, compared to baseline activities speaking, yawning, and resting. 

To generate data that is relatively close to the actual distribution, we designed the routines as naturally as possible. For example, the experiments were conducted at different intervals, e.g., participants had to yawn 20 times for three seconds in a row instead of once for 40 seconds. On the other hand, talking was done for a longer period. 

We achieved good classification results across all activities. This result was somewhat expectable as Knierim et al. conducted a comparative investigation. Consistent with our findings was furthermore the fact that activities involving larger facial muscles tend to have better prediction scores. The immaculate classification of eating soup could be explained by using a spoon and opening our mouths relatively wide. Since we ate tomato soup without any solid ingredients, we did not chew to swallow. Soup and sandwiches also show good separation. While taking a bite from a sandwich, nuts can be chewed directly with the back teeth. In addition, to take a bite, it is usually required to open the mouth more widely than when eating nuts. 
As shown in the confusion matrix, Random Forest confused baseline activities frequently. For example, speaking activities were classified as resting with closed eyes. Movements could cause this during the resting phase as the cEEGrids recognize even slight motions, which could have distorted the data on resting. Even in a controlled environment, it is complicated to prevent subjects from moving. However, it represents natural behavior. 

With our findings, we were able to tackle limitations of related experiments. In contrast to the work of and, in which food intake was classified by sounds, our approach is not distorted by external noise. In addition, we have shown that it is possible to classify different foods with cEEGrids.
Nonetheless, our approach could not eliminate all limitations from prior research. It is costly for people to turn their devices on and off before eating. In our setup people need to wear an around-the-ear electrode array, to gather data. This is not yet practical and requires further improvements. In summary, our machine learning model can already classify a limited number of different foods. However, it is debatable if the sheer endless food possibilities can be distinguished with this approach with the needed accuracy. Therefore, a combination with other techniques is recommended. For example, it would be conceivable to supplement the data with images taken via smart glasses.

However, it might not be necessary to classify which food is eaten exactly to generate insights into eating patterns. Therefore the model could already be used to enrich and enhance obesity treatments. A possible application could be the classification of food and fluids compared to not eating and drinking at all. In nursing homes, food intake is measured by 24-Hour dietary recall, food frequency questionnaires or biochemical markers. Recall and questionnaires are subjective and prone to errors, particularly when completed by elderly people in need of care.

Since we designed the experiment to set the environment as natural as possible, the routines have different lengths. Since we cut the data in one-second epochs, we received an unbalanced data set during feature engineering. The problem with an unbalanced dataset is that the classifier tends to focus its attention on the majority classes, which often leads to bad classification results for the minority class. To overcome this challenge, we oversampled the minority classes with SMOTE. However, synthetically generated data does not necessarily represent the actual underlying distribution, which can bias the result and lead to a model not being able to generalize on real data. Additionally, oversampling can lead to more noisy data, which can result in an overfitting classifier.

As we focused on the general performance of different classifiers, we did not optimize parameters for different classifiers. Therefore classifiers could lead to different results when applying parameter optimization. MLPs are known for their high capacity and state-of-the-art performance on complex problems. However, they require a large amount of training data. The generated data set might not have been large enough for MLPs to learn the underlying pattern, impacting their performance negatively. Especially for small data sets, the generalization can be affected negatively since it could be influenced by the selection of training and test data. Although we tackled this problem with the 5-fold cross-validation, there could be a significant improvement in the classification results when extending observations.

Even though the Random Forest classifier already performed remarkably well, performance could still be improved. We did not distinguish between features' relevance; those with a low information gain can harm the overall performance. Moreover, unnecessary features increase the size of possible Decision Trees, which correlates with the computational costs. In conclusion, the Random Forest classifier could yield even higher performance when taking information gain of features into account. The scores of the 5-fold cross-validated models indicate a very stable and generalized performance. Nevertheless, it has to be taken into account that the data set is not representative since only two participants conducted the experiment and people from different cultural backgrounds form individual eating habits.

In our research, we combined the data sets of each subject. Another approach for further research could be the separation of the subjects. The classifiers could be trained using the data set of one participant to predict the outcome of another participant. This approach could 
provide information about how well models can generalize.

Besides selecting the best model, calorie approximation also remains unsolved. It is still not evaluated how calories will be mapped to the eating activities. Carrots and nuts, for example, may contract similar muscles, which could lead to a similar classification. The confusion could be problematic, as they differ significantly in calorie density.

As described earlier, the current hardware is neither comfortable to wear nor easy to set up. However, powerful applications can emerge when sensors become applicable for everyday use. It is conceivable to set up a training pipeline where the sensor is worn in everyday life. By establishing an interface between the subject, EEG Sensor, and the machine learning model, the participant can provide information on what he is currently doing or eating. Extensive amounts of labeled training data can be generated moderately fast and at relatively low costs by leveraging this human-machine-collaboration approach.
The model can continuously be updated in the background with the newly generated data and constantly expand its classification classes.

## Conclusion and Outlook
This work investigated the potential of using cEEGrids to classify eating behavior and distinguish it from other facial activities. To tackle the problem of objectively measuring individuals' food intake (behavior). Obesity is a disease that is becoming more widespread in developed societies and is putting increasing pressure on the health insurance system. Current obesity treatments still relies on data collection methods characterized by subjectivity and offer room for improvement.
 The experiment showed promising results. Random Forests showed to be the best performing model, achieving an f1-score of 85.9\% and an accuracy of 85.7\%. Further, even detecting different kinds of foods (nuts, sandwiches, and soup) was possible with very high accuracy. The classifier separated the eating activities almost perfectly, with the worst confusion rate at only 0.04, which occurred between eating soup and eating sandwiches. The high accuracy is impressive considering the short data collection period. Although the cEEGrids can already be worn unobtrusive and non-restrictive, the current form is not yet practical enough for daily and especially not flexible use.
Further research should therefore investigate decreasing the size of cEEGrids and making the installation more accessible, so they can be worn in everyday life while still producing reliable measurements. Further, one must reflect that our experiment was conducted in a closed environment and produced a data set, which only contains a limited range of activities. Hence the data set should first be supplemented with a broader range of activities and subjects to evaluate the application in the natural habitat of a human being. Further, it is not yet clear how different granular types of food can be distinguished. Illustrated by an example, it might be challenging to distinguish carrots from eating nuts. However, these two activities differ significantly in the number of calories.
Nevertheless, there is much potential to make the measurement of calorie intake less subjective using cEEGrids, for which this paper can play a small part. 

Furthermore, the current functionality could be sufficient to detect unhealthy patterns in the food intake of overweight patients. For example, it would be conceivable to monitor eating behavior throughout the day to detect that the person eats conspicuously much at work, eats very frequently during the day in general, or always eats quite late in the evening.
Other use cases emerge when taking a somewhat different view away from obesity towards other diseases.
People with dementia often suffer from a deregulated food intake since they sometimes no longer know when to eat and whether they have already eaten or drunk. They then cook, eat and drink less (or even more), leading to malnutrition (occasionally overeating) or lack of fluids. Depending on the living situation, patients themselves could be reminded to eat or drink something, or the attending person could be made aware Other applications could lay in the treatment of anorexia. For example, detecting generic eating events or patterns could supply valuable data and insights to treating physicians or individuals and improve treatment. 

As expressed, a deviation from regular food intake in either direction is potentially harmful to health. In both cases, objective monitoring of food intake can improve the situation.

